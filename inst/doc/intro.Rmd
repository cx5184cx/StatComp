---
title: "Introduction to StatComp20011"
author: "Jiaqi Wu"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__StatComp20011__ is a simple R package to carry out EM algorithm on bivariate normal dataset with missing. Three functions are considered, namely, _criterion_ (stopping criterion), _my.em_ (EM algorithm) and _my.boot_(bootstrp algorithm). The package has a built-in dataset _bvn_ and we will test on it.

## Problem description and solving

Suppose that $Y=\left(Y_{1}, Y_{2}\right)^{T},$ where $Y_{1}, Y_{2}$ are scalar outcomes that can be missing at random. Assume that $Y$ is bivariate normal, which we write as
$$
Y \sim \mathcal{N}(\mu, \Sigma), \quad \mu=\left(\mu_{1}, \mu_{2}\right)^{T}, \quad \Sigma=\left(\begin{array}{cc}
\sigma_{1}^{2} & \sigma_{12} \\
\sigma_{12} & \sigma_{2}^{2}
\end{array}\right).
$$

Define $R=\left(R_{1}, R_{2}\right)$ as missing indicator, we can see that there are three possible values $r$ of $R:$

(1) $Y_{1}, Y_{2}$ are both observed, $r=(1,1), Y_{(r)}=Y$.

(2) $Y_{1}$ is observed, $Y_{2}$ is missing, $r=(1,0), Y_{(r)}=Y_{1}$.

(3) $Y_{1}$ is missing, $Y_{2}$ is observed, $r=(0,1), Y_{(r)}=Y_{2}$.

Interest focused on estimation of $\theta$ based on the observed data. We will use EM algorithm to solve it. It is easy to see that $Y$ belongs to the exponential family, where the dimension of natural parameters is $p=5$ and it can be proved that we only needs to consider sufficient statistics while being exponential family distribution in EM algorithm as:

E-step: Find $\quad E_{\theta^{(t)}}\left\{T_{\ell}(Y)|R, Z_{(R)}\right\}, \quad \ell=1, \ldots, p.$

M-step: Find $\theta^{(t+1)}$ as the solution to the $p$ estimate equations
$$
T_{\ell}(Y)=E_{\theta^{(t)}}\left\{T_{\ell}(Y)|R, Y_{(R)}\right\}, \quad \ell=1, \ldots, p.
$$
It can be shown that the $p=5$ sufficient statistics are
$T_{1}(Y)=\sum_{i=1}^{N} Y_{i 1}, \quad T_{2}(Y)=\sum_{i=1}^{N} Y_{i 1}^{2}, \quad T_{3}(Y)=\sum_{i=1}^{N} Y_{i 2}, \quad T_{4}(Y)=\sum_{i=1}^{N} Y_{i 2}^{2}, \quad T_{5}(Y)=\sum_{i=1}^{N} Y_{i 1} Y_{i 2}.$

Thus, the estimate equations yield
$$
\begin{aligned}
\mu_{1} &=N^{-1} \sum_{i=1}^{N} Y_{i 1}, \quad \sigma_{1}^{2}=N^{-1} \sum_{i=1}^{N} Y_{i 1}^{2}-\left(\mu_{1}\right)^{2}, \quad \mu_{2}=N^{-1} \sum_{i=1}^{N} Y_{i 2}, \quad \sigma_{2}^{2}=N^{-1} \sum_{i=1}^{N} Y_{i 2}^{2}-\left(\mu_{2}\right)^{2}, \\
\sigma_{12} &=N^{-1} \sum_{i=1}^{N} Y_{i 1} Y_{i 2}-\mu_{1} \mu_{2},
\end{aligned}
$$

$Y_{i}, i=1, \ldots, N,$ and $\left(R_{i}, Y_{\left(R_{i}\right) i}\right), i=1, \ldots, N,$ are i.i.d., the E-step involves finding
$$
E\left(Y_{1}|Y_{(r)}\right), \quad E\left(Y_{1}^{2} |Y_{(r)}\right), \quad E\left(Y_{2} |Y_{(r)}\right), \quad E\left(Y_{2}^{2}|Y_{(r)}\right), \quad E\left(Y_{1} Y_{2}|Y_{(r)}\right)
$$
for the three cases above. 

Trivially, in case 1, $r=(1,1),$ the conditional expectations are equal to the observed values $Y_{1}, Y_{1}^{2}, Y_{2}, Y_{2}^{2}, Y_{1} Y_{2} .$ 

For case 2 $, r=(1,0), Z_{(r)}=Y_{1}$,
$$
E\left(Y_{1} |Y_{(r)}\right)=Y_{1}, \quad E\left(Y_{1}^{2} |Y_{(r)}\right)=Y_{1}^{2},
$$
while, by standard properties of the multivariate normal distribution,
$$
\begin{array}{c}
E\left(Y_{2} |Y_{(r)}\right)=E\left(Y_{2} |Y_{1}\right)=\mu_{2}+\sigma_{12}\left(Y_{1}-\mu_{1}\right) / \sigma_{1}^{2}, \\
E\left(Y_{2}^{2} | Y_{(r)}\right)=E\left(Y_{2}^{2} |Y_{1}\right)=\sigma_{2}^{2}-\sigma_{12}^{2} / \sigma_{1}^{2}+\left\{E\left(Y_{2} | Y_{1}\right)\right\}^{2}, \\
E\left(Y_{1} Y_{2} | Y_{(r)}\right)=E\left(Y_{1} Y_{2}| Y_{1}\right)=Y_{1} E\left(Y_{2}| Y_{1}\right).
\end{array}
$$
For case 3, $r=(0,1),$ reverse the roles of $Y_{1}$ and $Y_{2}$ in these expressions.

Combining, the EM-algorithm takes the following form. At the $(t+1)$ iteration:

E-step: With $\theta^{(t)}=\left(\mu_{1}^{(t)}, \sigma_{1}^{2(t)}, \mu_{2}^{(t)}, \sigma_{2}^{2(t)}, \sigma_{12}^{(t)}\right)^{T},$ calculate
$$
\begin{array}{c}
T_{1}^{(t)}=\sum_{i=1}^{N} \sum_{r} I\left(R_{i}=r\right) E_{\theta^{(t)}}\left(Y_{i 1} | Y_{(r) i}\right), \quad T_{2}^{(t)}=\sum_{i=1}^{N} \sum_{r} I\left(R_{i}=r\right) E_{\theta^{(t)}}\left(Y_{i 1}^{2} | Y_{(r) i}\right), \\
T_{3}^{(t)}=\sum_{i=1}^{N} \sum_{r} I\left(R_{i}=r\right) E_{\theta^{(t)}}\left(Y_{i 2} |Y_{(r) i}\right), \quad T_{4}^{(t)}=\sum_{i=1}^{N} \sum_{r} I\left(R_{i}=r\right) E_{\theta^{(t)}}\left(Y_{i 2}^{2} |Y_{(r) i}\right), \\
T_{5}^{(t)}=\sum_{i=1}^{N} \sum_{r} I\left(R_{i}=r\right) E_{\theta^{(t)}}\left(Y_{i 1} Y_{i 2} | Y_{(r) i}\right).
\end{array}
$$

M-step: Update $\theta^{(t+1)}$ as
$$
\mu_{1}^{(t+1)}=T_{1}^{(t)} / N, \quad \sigma_{1}^{2(t+1)}=T_{2}^{(t)} / N-\left(\mu_{1}^{(t+1)}\right)^{2}, \quad \mu_{2}^{(t+1)}=T_{3}^{(t)} / N, \quad \sigma_{2}^{2(t+1)}=T_{4}^{(t)} / N-\left(\mu_{2}^{(t+1)}\right)^{2},
$$
$$
\sigma_{12}^{(t+1)}=T_{5}^{(t)} / N-\left(\mu_{1}^{(t+1)} \mu_{2}^{(t+1)}\right).
$$

## Dataset 

The dataset _bvn_ is simulated from "randomized monotone missingness process" in Robins and Gill (1997), Statistics in Medicine. The R code for the simulation is as follows:
```{r,eval=FALSE}
set.seed(4)
N <- 1000
theta <- c(5,8,1,0.5,1)
mu <- theta[1:2]
sigma <- matrix(c(theta[3],theta[4],theta[4],theta[5]),2,2)
prob <- 0.6     
#  prob observe y1
psi1 <- c(-1,0.3)
psi2 <- c(-1,0.1)
x <- NULL
r <- NULL

for (i in 1:N){
  obs <- rbinom(1,1,prob)        
  #  observation indicator 
  y <- mu + t(chol(sigma))%*%rnorm(2,0,1)
  #  Generate full data
  ep1 <- exp(psi1[1]+psi1[2]*y[1])
  p1 <- ep1/(1+ep1)
  ep2 <- exp(psi2[1]+psi2[2]*y[2])
  p2 <- ep2/(1+ep2)
  r1 <- obs + (1-obs)*rbinom(1,1,p2)
  r2 <- (1-obs) + obs*rbinom(1,1,p1)  
  x <- rbind(x,t(y))
  r <- rbind(r,c(r1,r2))
}
x <- round(x,3)    
#  Round to 3 decimal to make it like real data
x[r==0] <- NA
#  Replacing missing with NA
bvn <- x 
save(bvn,file="bvn.rda")  
```



## EM algorithm

The source R code for _my.em_ is as follows:
```{r,eval=FALSE}
my.em <- function(x,max=100,tol=1e-5,pr){
  r <- 1-is.na(x)
  #  missing index
  N <- nrow(x)
  iter <- exist <- 1
  xc <- x[complete.cases(x), ]
  #  complete case
  Nc <- nrow(xc)
  x[is.na(x)] <- -999
  #  Change NAs to nonsensical value
  mu0 <- apply(xc,2,mean)
  sigma0 <- t(sweep(data.matrix(xc),2,mu0))%*%sweep(data.matrix(xc),2,mu0)/(Nc-1)
  #  complete case sample mean and sample covariance as initial
  theta <- c(mu0,sigma0[1,1],sigma0[1,2],sigma0[2,2])
  record <- c(0,theta)
  #  record
  while (! criterion(iter,max,tol,exist)){
    mu <- theta[1:2]
    sigma <- matrix(c(theta[3],theta[4],theta[4],theta[5]),2,2)
    ey1 <- (r[,1]==1)*x[,1] + (r[,1]==0)*(mu[1]+ sigma[1,2]*(x[,2]-mu[2])/sigma[2,2])
    ey2 <- (r[,2]==1)*x[,2] + (r[,2]==0)*(mu[2]+ sigma[1,2]*(x[,1]-mu[1])/sigma[1,1])
    ey1.2 <- (r[,1]==1)*x[,1]^2 + (r[,1]==0)*(sigma[1,1]-sigma[1,2]^2/sigma[2,2]+ ey1^2)
    ey2.2 <- (r[,2]==1)*x[,2]^2 + (r[,2]==0)*(sigma[2,2]-sigma[1,2]^2/sigma[1,1]+ ey2^2)
    ey1y2 <- (r[,1]*r[,2]*x[,1]*x[,2] + r[,1]*(1-r[,2])*x[,1]*ey2
              + (1-r[,1])*r[,2]*ey1*x[,2])
    #  E-Step
    munew <- c(mean(ey1),mean(ey2))
    s11 <- mean(ey1.2)-munew[1]^2
    s22 <- mean(ey2.2)-munew[2]^2
    s12 <- mean(ey1y2)-munew[1]*munew[2]
    #  M-Step
    thetanew <- c(munew,s11,s12,s22)
    if (pr==1) print(c(iter,thetanew))
    #  print the results of each iteration
    record <- rbind(record,c(iter,thetanew))
    #  record
    exist <- (thetanew-theta)/theta
    #  relative change
    theta <- thetanew
    iter <- iter+1
  }
  results <- list(theta.em=theta,record=record)
  return(results)
}
```

We use bootstrap to get standard errors for $\hat{\theta}$. The source code is as follows:
```{r,eval=FALSE}
my.boot <- function(x,max=100,tol=1e-5,B){
  N <- nrow(x)
  theta.boot <- NULL
  for (b in 1:B){
    i <- sample(N,N,replace=TRUE)
    #  Sample with replacement
    xc.b <- x[i,]
    #  Call EM
    em.b <- my.em(xc.b,max,tol,0)
    theta.b <- em.b$theta.em
    theta.boot <- rbind(theta.boot,theta.b)
    
  }
  boot.mean <- apply(theta.boot,2,mean)
  boot.se <- apply(theta.boot,2,sd)
  return(list(boot.mean=boot.mean,boot.se=boot.se))
}
```

We will illustrate EM algorithm use _my.em_ and _my.boot_ with _bvn_ as follows:
```{r}
set.seed(4) 
library("StatComp20011")
data(bvn)
pr <- 1
max <- 100
tol <- 1e-5
a <- my.em(bvn,max,tol,pr)$theta.em
b <- my.boot(bvn,max,tol,250)$boot.se
df <- data.frame(a,b)
names(df) <- c("mean", "standard error")
rownames(df) <- c("mu1","mu2","sigma[1,1]","sigma[1,2]","sigma[2,2]")
knitr::kable(df)
```

Below, we will use norm package to compare as follows:
```{r}
library(norm)
prelim.em <- prelim.norm(bvn)
#  set up the data
theta <- em.norm(prelim.em,showits=FALSE)
theta <- getparam.norm(prelim.em,theta,corr=TRUE)
theta$mu <- c(5.042471,7.952805)
theta$sdv <- sqrt(c(1.026614,1.033791))
theta$r[1,2] <- 0.494137/sqrt(1.026614*1.033791)
#  Use the sample complete case standard deviations
#  Use the sample complete case correlation
theta <- makeparam.norm(prelim.em,theta)
theta.final <-
  em.norm(prelim.em,start=theta,showits=TRUE,maxits=100,criterion=1e-5)        
theta.final <- getparam.norm(prelim.em,theta.final,corr=TRUE)
mu.final <- theta.final$mu
sd.final <- theta.final$sdv
var.final <- sd.final^2
covar.final <- theta.final$r[1,2]*sd.final[1]*sd.final[2]
round(c(mu.final,var.final[1],covar.final,var.final[2]),6)
```

We can see that the function we write gets simliar results with functions in package norm.  