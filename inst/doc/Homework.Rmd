---
title: "Homework"
author: "Jiaqi Wu"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
## Homework-09.22
## Example1
Below are the scatter plots of Violent Crime Rates by US State:
```{R}
pairs(USArrests, pch=23, bg='orange',                                          
      cex.labels=1.5) 
```

## Example2
Below is the head of the data:
```{r, results = 'asis'}
knitr::kable(head(USArrests))
```

## Example3

Here are a few facts about multiple linear regression:
$$Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip} + \varepsilon_i$$
 $$\begin{aligned}
       SSE(\beta_0, \dots, \beta_p) &= \sum_{i=1}^n\left(Y_i - \left(\beta_0 + \sum_{j=1}^p \beta_j \
X_{ij} \right) \right)^2 \\
       &= \|Y - \widehat{Y}(\beta)\|^2
       \end{aligned}$$
$$\widehat{\sigma}^2 = \frac{SSE}{n-p-1} \sim \sigma^2 \cdot \frac{\chi^2_{n-p-1}}{n-p\
-1}$$

## Homework-09.29
## Question1
The Pareto $(a, b)$ distribution has cdf
$$
F(x)=1-\left(\frac{b}{x}\right)^{a}, \quad x \geq b>0, a>0.
$$
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample from the Pareto$(2, 2)$ distribution. Graph the density histogram of the sample with the Pareto $(2,2)$ density superimposed for comparison.

## Answer
If $X\sim Pareto(2, 2)$, then for $x>0$ the cdf of $X$ is $F_X(x)=1-\left(\frac{2}{x}\right)^{2}$. The inverse transformation is $F_X^{-1}(u)=\frac{2}{\sqrt{1-u}}$. Note $U$ and $1-U$ have the same distribution, we generate $X$ with $x= \frac{2}{\sqrt{u}}$.
```{R}
set.seed(12345)
n <- 10000
u <- runif(n)
x <- 2/sqrt(u)
hist(x, prob=TRUE, main = 'Pareto(2,2)', xlim = c(0,50), breaks=500)
y <- seq(2, 50, .1)
lines(y, 8/(y^3),lwd = 1.5, col = "red")
```

We can see that the empirical and theoretical distributions approximately agree.

## Question2
The rescaled Epanechnikov kernel is a symmetric density function
$$
f_{e}(x)=\frac{3}{4}\left(1-x^{2}\right), \quad|x| \leq 1.
$$
Devroye and Györfi give the following algorithm for simulation from this distribution. Generate iid $U_{1}, U_{2}, U_{3} \sim$ Uniform $(-1,1) .$ If $\left|U_{3}\right| \geq$ $\left|U_{2}\right|$ and $\left|U_{3}\right| \geq\left|U_{1}\right|,$ deliver $U_{2} ;$ otherwise deliver $U_{3} .$ Write a function to generate random variates from $f_{e},$ and construct the histogram density estimate of a large simulated random sample.

## Answer
```{R}
repan <- function(n){
   u <- runif(3*n, min = -1, max = 1)
for(i in 1:n){
   if(abs(u[3*i]) >= abs(u[3*i-1]) && abs(u[3*i]) >= abs(u[3*i-2])) x[i] <- u[3*i-1] else x[i] <- u[3*i]
}
   return(x)
}
set.seed(12345)
x <- repan(10000)
hist(x, prob=TRUE, main = 'Epanechnikov kernel', breaks = 50)
```

## Question3
Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_e$.

## Answer
Consider this alternative description of the same algorithm:

1.Generate iid $U_1,U_2,U_3$ with Uniform$(0,1)$ distributions.

2.Select one of the two smallest of the $U_i$ at random, with equal probability. Call this value X.

3.Randomly negate X with probability $\frac 1 2$.

Note that X is indeed distributed from the mixture of the distributions of the first order statistic and of the second order statistic for three uniforms on (0,1).Thus,

$$f_X(x)=\frac1 2 f_{U_{(1)}}(x)+\frac1 2 f_{U_{(2)}}(x)=\frac1 2 3(1-x)^2+\frac1 2 6x(1-x)=\frac3 2-\frac3 {2}x^2.$$
According to the third step in the alternative description, extending f to the domain $[−1,1]$ by symmetry. As $(-x)^2=x^2$, we can get $f_e(x)$ with
$$f_e(x)=\frac1 2 f_X(x)=\frac{3}{4}\left(1-x^{2}\right).$$

This completes the proof.


## Question4
It can be shown that the mixture in Exercise 3.12 has a Pareto distribution with cdf
$$
F(y)=1-\left(\frac{\beta}{\beta+y}\right)^{r}, \quad y \geq 0.
$$
Generate 1000 random observations from the mixture with $r=4$ and $\beta=2 .$ Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.

## Answer
```{R}
set.seed(12345)
n <- 1000
u <- runif(n)
x <- 2*(1-u^(1/4))/u^(1/4)
hist(x, prob=TRUE, main = 'Pareto', xlim = c(0,10), breaks=100)
y <- seq(0, 10, .1)
lines(y, 64/(2+y)^5,lwd = 1.5, col = "red")
```

We can see that the empirical and theoretical distributions approximately agree.

## Homework-10.13
## Question1
Compute a Monte Carlo estimate of
$$
\int_{0}^{\pi / 3} \sin t d t,
$$
and compare your estimate with the exact value of the integral.

## Answer
We can straightly compute $\int_{0}^{\pi / 3} \sin t d t=-\left.\cos t\right|_{0} ^{\pi / 3}=\frac 1 2$. Thus the exact value is 0.5.
```{R}
set.seed(12345)
n <- 1e4
x <- runif(n, min = 0, max = pi/3)
theta.hat <- mean(sin(x))*pi/3
print(theta.hat)
print(1/2)
```

We can see that the estimate is close to the exact value.

## Question2
Refer to Exercise $5.6 .$ Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6 .

## Answer
We first compute the theoretical percent reduction in variance as above:
$$\frac{Var(e^U)-Var(\frac{e^U+e^{1-U}}{2})}{Var(e^U)}=\frac{\frac 1 2 Var(e^U)-\frac 1 2Cov(e^U,e^{1-U})}{Var(e^U)}=\frac1 2 -\frac{e-(e-1)^2}{2(\frac{e^{2}-1}{2}-(e-1)^{2})}\doteq0.983835.$$
```{R}
set.seed(12345)
m <- 100000
U <- runif(m)
T1 <- exp(U)
T2 <- 1/2*exp(U)+1/2*exp(1-U)
mean(T1)
mean(T2)
(var(T1)-var(T2))/var(T1)
```
We can see that empirical estimate of the percent reduction in variance is similiar to the theoretical.


## Question3
If $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are unbiased estimators of $\theta,$ and $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are antithetic, we derived that $c^{*}=1 / 2$ is the optimal constant that minimizes the variance of $\hat{\theta}_{c}=c \hat{\theta}_{1}+(1-c) \hat{\theta}_{2} .$ Derive $c^{*}$ for the general case. That is, if $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$
are any two unbiased estimators of $\theta,$ find the value $c^{*}$ that minimizes the variance of the estimator $\hat{\theta}_{c}=c \hat{\theta}_{1}+(1-c) \hat{\theta}_{2}$ in equation $(5.11) .\left(c^{*}\right.$ will be a function of the variances and the covariance of the estimators.)

## Answer
Note that 
$$Var(\hat{\theta}_c)=Var\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)c^{2} +2 Cov\left(\hat{\theta}_{2}, \hat{\theta}_{1}-\hat{\theta}_{2}\right)c +Var\left(\hat{\theta}_{2}\right).$$
Thus the quadratic function of $c$ arrives its minimum at $$c^*=-\frac {Cov\left(\hat{\theta}_{2}, \hat{\theta}_{1}-\hat{\theta}_{2}\right)}{Var\left(\hat{\theta}_{1}-\hat{\theta}_{2}\right)}=\frac{Var(\hat{\theta}_{2})-Cov(\hat{\theta}_{1},\hat{\theta}_{2})}{Var(\hat{\theta}_{1})-2Cov(\hat{\theta}_{1},\hat{\theta}_{2})+Var(\hat{\theta}_{2})}.$$

## Homework-10.20
## Question1
Find two importance functions $f_{1}$ and $f_{2}$ that are supported on $(1, \infty)$ and are 'close' to
$$
g(x)=\frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}, \quad x>1,
$$
Which of your two importance functions should produce the smaller variance in estimating
$$
\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x,
$$
by importance sampling? Explain.

## Answer
Let $$
\begin{aligned}
&f_{1}(x)=e^{1-x}, \quad 1<x<\infty,\\
&f_2(x)=4(1+x^2)^{-1}/\pi, \quad 1<x<\infty.
\end{aligned}
$$
We will use inverse transform method to generate from $f_1(x)$ and $f_2(x)$.
```{r}
n <- 10000
set.seed(12345)
theta_hat <- numeric(2)
se <- numeric(2)
g <- function(x){
  x^2*exp(-x^2/2)/(sqrt(2*pi))* (x > 1)
}

u <- runif(n) #f1,inverse transform method
x <- 1-log(u)
fg <- g(x)/exp(1-x)
theta_hat[1] <- mean(fg)
se[1] <- sd(fg)

u <- runif(n) #f2,inverse transform method
x <- tan(pi/4*(u+1))
fg <- g(x)/(4/((1+x^2)*pi))
theta_hat[2] <- mean(fg)
se[2] <- sd(fg)

rbind(theta_hat, se)
```
Note that $f_1$ has smaller variance because $f_1$ has a more similiar shape to g. We will illustrate it below:
```{r}
x <- seq(1, 10, 0.01)
w <- 2
f1 <- exp(1-x)
f2 <- 4/((1+x^2)*pi)
g <- x^2/(sqrt(2*pi)) * exp(-x^2/2)

plot(x, g, type = "l", ylim=c(0,1.5), lwd = w)
lines(x, f1, lty = 2, lwd = w)
lines(x, f2, lty = 3, lwd = w) 
legend("topright", legend = c("g", 1:2), lty = 1:3, lwd = w, inset = 0.02)

plot(x, g/f1, type = "l", ylim = c(0,3), lwd = w, lty = 2)
lines(x, g/f2, lty = 3, lwd = w)
legend("topright", legend = c(1:2), lty = 2:3, lwd = w, inset = 0.02)
```

We can see from the picture that $f_1$ is more similiar to g.

## Question2
5.15 Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

## Answer
```{r 5.15}
n <- 10000
k <- 5
r <- n/k
N <- 50
set.seed(12345)

T2 <- numeric(k)
est <- matrix(0, N, 2)
g<-function(x)exp(-x)/(1+x^2)*(x>0)*(x<1)
for (i in 1:N){
  u <- runif(n)
  x <- - log(1 - u * (1 - exp(-1)))
  fg <- g(x) / (exp(-x) / (1 - exp(-1)))
  est[i, 1] <- mean(fg)
  for(j in 1:k){ 
    u <- runif(n/k)
    t <- u * exp(-j/5)+(1 - u) * exp(-(j-1)/5)
    x <- - log(t)
    fg <- 5*g(x) / (exp(-x) / (exp(-(j-1)/5) - exp(-j/5)))
    T2[j] <- mean(fg)
  }
  est[i, 2] <- mean(T2)
} 
theta_hat <- apply(est,2,mean)
se <- apply(est,2,sd)
rbind(theta_hat, se)
```
We can see that the standard error in the stratified sampling is approximately $\frac 1 5$ of the 
importance sampling. The same as theoretical result that the standard error in the stratified sampling
is reduced to $\frac 1 k$ of the importance sampling case.

## Question3
Suppose that $X_1,\cdots,X_n$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a $95$% confidence interval for the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level. 

## Answer
Assume that $X_1,\cdots,X_n \sim X$ and $lnX \sim N(\mu,\sigma^2)$, thus we have $lnX_1,\cdots,lnX_n \sim N(\mu,\sigma^2)$,
$$
\frac{\overline{lnX}-\mu}{S/\sqrt{n}}\sim t_n,
$$
where $S^2=\frac 1{n-1}\sum_{i=1}^n(lnX_i-\overline{lnX})^2$, thus we get the confidence interval,
$$
\left(\frac{1}{n} \sum_{i=1}^n \ln {X_i} - t_n(1-\alpha/2) S / \sqrt{n}, \frac{1}{n} \sum_{i=1}^n \ln {X_i} + t_n(1-\alpha/2) S / \sqrt{n}\right),
$$
where $\alpha=0.05$.
We use $$
P(\left|\frac{\overline{lnX}-\mu}{S/\sqrt{n}}\right| < t_n(1-\alpha/2))=P(\left|\frac{\overline{lnX}-\mu}{t_n(1-\alpha/2)S/\sqrt{n}}\right| < 1)=1-\alpha,
$$
to generate empirical estimate of the confidence level.
```{r}
n <- 20
alpha <- .05
set.seed(1)
UCL <- replicate(1000, expr = {
  x <- rlnorm(n, 0, 2)
  sqrt(n)*mean(log(x))/sqrt(var(log(x)))/ qt(1-alpha/2,n-1)
  })
mean(abs(UCL) < 1)
```
We can see that empirical estimate of the confidence level is 0.954, very close to the true value.

## Question4
Suppose a $95 \%$ symmetric $t$ -interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to $0.95 .$ Use a Monte Carlo experiment to estimate the coverage probability of the $t$ -interval for random samples of $\chi^{2}(2)$ data with sample size $n=20 .$ Compare your $t$ -interval results with the simulation results in Example $6.4 .$ (The $t$ -interval should be more robust to departures from normality than the interval for variance.)

## Answer
We generate the empirical estimate of the confidence level the same as question $3$ and example $6.4$.
```{r}
n <- 20
alpha <- .05
UCL <- replicate(1000, expr = {
  x <- rchisq(n, df = 2)
  sqrt(n)*(mean(x)-2)/sqrt(var(x))/ qt(1-alpha/2,n-1)
  })
mean(abs(UCL) < 1)

UCL <- replicate(1000, expr = {
  x <- rchisq(n, df = 2)
  (n-1)*var(x)/qchisq(alpha,n-1)
  })
mean(UCL > 4)
```
We can see that coverage probability of the $t$ -interval is much more close to 0.95 than the interval of the variance. This proves that $t$ -interval is more robust to departures from normality than the interval for variance.

## Homework-10.27
## Question1
 Estimate the power of the skewness test of normality against symmetric Beta($\alpha,\alpha$) distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(\nu)$?

## Answer
```{r}
sk <- function(x) {
#computes the sample skewness coeff.
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}

set.seed(12345)
a <- .1              #significance level
n <- 30 
m <- 3000
alpha <- seq(.1, 20.1, 1)
N <- length(alpha)
pwr_Beta <- numeric(N)
#critical value for the skewness test
cv <- qnorm(1 - a / 2, 0, sqrt(6 * (n - 2) / ((n + 1) * (n + 3))))

for (i in 1:N) {     #for each alpha 
  sktests_Beta <- numeric(m)
  for (j in 1:m) {   #for each replicate
    x_Beta <- rbeta(n, alpha[i], alpha[i])
    sktests_Beta[j] <- as.integer(abs(sk(x_Beta)) >= cv)
  }
  pwr_Beta[i] <- mean(sktests_Beta)
}
#plot power vs alpha 
plot(alpha, pwr_Beta, xlab = bquote(alpha), ylab = "power",
      type = "b", ylim = c(0, 0.1))
abline(h = .1, lty = 3)                         #significance level
se_Beta <- sqrt(pwr_Beta * (1 - pwr_Beta) / m)  #standard error
#plot confidence interval
lines(alpha, pwr_Beta + se_Beta, lty = 3)
lines(alpha, pwr_Beta - se_Beta, lty = 3)
```

The empirical power curve vs $\alpha$ is shown above. Note that the power curve is below the horizontal line corresponding to a=0.10. This is beacause the skewness of points generated from symmetric beta is always near 0. That makes the test inefficient in the symmetric beta circumstance.
```{r}
nu <- seq(.1, 20.1, 1)
N2 <- length(nu)
pwr_t <- numeric(N2)

for (i in 1:N2) {        #for each nu 
  skt_t <- numeric(m)
  for (j in 1:m) {       #for each replicate
    x_t <- rt(n, nu[i])
    skt_t[j] <- as.integer(abs(sk(x_t)) >= cv)
  }
  pwr_t[i] <- mean(skt_t)   #power
}

#plot power vs nu
plot(nu, pwr_t, xlab = bquote(nu), ylab = "power", 
     type = "b", ylim = c(0, 1))
abline(h = .1, lty = 3)                #significance level
se_t <- sqrt(pwr_t * (1 - pwr_t) / m)  #standard error
#plot confidence interval
lines(nu, pwr_t + se_t, lty = 3)
lines(nu, pwr_t - se_t, lty = 3)
```

The empirical power curve vs $\nu$ is shown above. Note that the power curve is above the horizontal line corresponding to a=0.10 and power gets smaller as $\nu$ gets bigger. This is because the t-distribution is similiar to normal distribution as nu gets big. Therefore, it is difficult to distinguish one from another and makes the power small. The power is big with small $\nu$.

All in all, it illustrates that the skewness test of normality is inefficient to symmetric beta and t-distribution with large $\nu$. We can only use the skewness test of normality to detect t-distribution with small $\nu$.

## Question2
Refer to Example 6.16. Repeat the simulation, but also compute the $F$ test of equal variance, at significance level $\hat{\alpha}\doteq0.055$. Compare the power of the Count Five test and $F$ test for small, medium, and large sample sizes. (Recall that the $F$ test is not applicable for non-normal distributions.)

## Answer
```{r}
sigma1 <- 1
sigma2 <- 1.5
m <- 10000

count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(as.integer(max(c(outx, outy)) > 5))
}

power_count <- function(m,n,sigma1,sigma2){
  tests <- replicate(m, expr = {
  x <- rnorm(n, 0, sigma1)
  y <- rnorm(n, 0, sigma2)
  count5test(x, y)
  } )
  return(mean(tests))
}

power_f <- function(m,n,sigma1,sigma2){
  tests <- replicate(m, expr = {
  x <- rnorm(n, 0, sigma1)
  y <- rnorm(n, 0, sigma2)
  return(as.integer(var.test(x,y,conf.level=0.945)$p.value<0.055))
  } )
  return(mean(tests))
}

n <- c(20, 100, 500)
k = length(n)
power_1 = numeric(k)
power_2 = numeric(k)

for(i in 1:k){
  power_1[i] <- power_count(m,n[i],sigma1,sigma2)
  power_2[i] <- power_f(m,n[i],sigma1,sigma2)
}
df<-data.frame(power_1, power_2)
names(df) <- c("Count Five test","F-test")
row.names(df) <- c("n=20", "n=100", "n=500")
knitr::kable(df)
```

We can see that the power of both tests increase as n gets bigger which means that both methods are asmyptotic efficient. And the power of F-test is better than Count Five test in all the circumstances.


## Question3
Repeat Examples 6.8 and 6.10 for Mardia's multivariate skewness test. Mardia proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$ and $Y$ are iid, the multivariate population skewness $\beta_{1, d}$ is defined by Mardia as
$$
\beta_{1, d}=E\left[(X-\mu)^{T} \Sigma^{-1}(Y-\mu)\right]^{3}.
$$
Under normality, $\beta_{1, d}=0 .$ The multivariate skewness statistic is
$$
b_{1, d}=\frac{1}{n^{2}} \sum_{i, j=1}^{n}\left(\left(X_{i}-\bar{X}\right)^{T} \widehat{\Sigma}^{-1}\left(X_{j}-\bar{X}\right)\right)^{3}.
$$
where $\hat{\Sigma}$ is the maximum likelihood estimator of covariance. Large values of $b_{1, d}$ are significant. The asymptotic distribution of $n b_{1, d} / 6$ is chisquared with $d(d+1)(d+2) / 6$ degrees of freedom.

## Answer
```{r}
set.seed(1234)
m <- 1000
library(MASS)
#compute the skewness function
sk2 <- function(X) {
        k <- nrow(X)
        Y <- matrix(rep(colMeans(X),k), nrow=k, byrow=TRUE)     #colmean 
        Sigma <- t(X-Y)%*%(X-Y)/k                               #covariance         
        b <- sum(((X-Y)%*%solve(Sigma)%*%t(X-Y))^3)
        return(b/k^2)
}
sktests=function(n,cv,d){
  tests <- replicate(m, expr = {                #tests
          x=mvrnorm(n,rep(0,d),diag(d))         #generation
          return(as.integer(sk2(x)>=cv)) 
          } )
mean(tests)
}
f <- function(d){
         n <- c(10,20,30,50,100,500)
         #critical value for the skewness test
         cv <- qchisq(0.95, d*(d+1)*(d+2)/6)*6/n
         p.reject <- numeric(length(n))
        for(i in 1:length(n)){
             p.reject[i] <- sktests(n[i],cv[i],d)
        }
 p.reject          #percent of reject
}
df2 <- data.frame(f(1), f(2))
names(df2) <- c("d=1","d=2")
row.names(df2) <- c("n=10", "n=20", "n=30", "n=50", "n=100", "n=500")
knitr::kable(df2)
```

From above we give the similiar result to exmaple 6.8 in dimension 1 and 2.
```{r}
alpha=0.1            #significance level
g <- function(d,n){
    epsilon <- c(seq(0, .15, .01), seq(.15, 1, .05))
    N <- length(epsilon)
    pwr <- numeric(N)
    #critical value for the skewness test
    cv <- qchisq(.95, d*(d+1)*(d+2)/6)*6/n  
    for (j in 1:N) {              #for each epsilon
        e <- epsilon[j]
        sktests <- numeric(m)
        for (i in 1:m) {           #for each replicate
                x <- matrix(0, nrow = n, ncol = d)
                for(k in 1:n){
                   sigma <- sample(c(1, 10), replace = TRUE,
                                size = 1, prob = c(1-e, e))        
                   x[k,] <- mvrnorm(1, rep(0, d), (sigma^2)*diag(d))     
                }
              sktests[i] <- as.integer(sk2(x)>=cv)  
        }
        pwr[j] <- mean(sktests)      #power
    }  
     #plot power vs epsilon
     plot(epsilon, pwr,xlab = bquote(epsilon), ylab = "power",
          type = "b", ylim = c(0,1))
     abline(h = .1, lty = 3)       #significance level
     se <- sqrt(pwr * (1-pwr) / m) #standard errors
     #plot confidence interval
     lines(epsilon, pwr+se, lty = 3)
     lines(epsilon, pwr-se, lty = 3) 
}
g(1,30) #d=1,n=30
g(2,30) #d=2,n=30
```

From above we plot the similiar picture to exmaple 6.10 in dimension 1 and 2.

## Question4
If we obtain the powers for two methods under a particular
simulation setting with 10,000 experiments: say, 0.651 for one
method and 0.676 for another method. Can we say the powers
are different at 0.05 level?

$(1)$ What is the corresponding hypothesis test problem?

$(2)$ What test should we use? Z-test, two-sample t-test, paired-t
test or McNemar test?

$(3)$ What information is needed to test your hypothesis?

## Answer
$(1)$ Let the tests be $T_1$ and $T_2$. Denote the power of $T_i$ as $\pi_i,$ $i=1,2$. Thus, the corresponding hypothesis test problem can be described as:$H_0:\pi_1=\pi_2\leftrightarrow H_a:\pi_1\neq\pi_2.$

$(2)$ As a matter of fact, Z-test,  paired-t test and McNemar test all can be used for test except two-sample t-test. This is because two-sample t-test requires that the data of two samples are statistically independent and this is not satisfied as we use the same simulation setting.

$(3)$ We need to know that in each simulation the null hypothesis was rejected or not for the two tests.

## Homework-11.03
## Question1
Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

## Answer
```{r}
data(law, package = "bootstrap") #import data
n <- nrow(law)
x <- law$LSAT
y <- law$GPA
theta_hat <- cor(x,y)
#compute the leave-one-out estimates
theta_jack <- rep(0, n)
for (i in 1:n)
  theta_jack[i] <- cor(x[-i],y[-i])
bias <- (n - 1) * (mean(theta_jack) - theta_hat)
se <- (n - 1) * sqrt(var(theta_jack) / n)
df <- data.frame(theta_hat, bias, se) 
knitr::kable(df)

```

## Question2
Refer to Exercise 7.4. Compute $95%$ bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

## Answer
Note that mle of $1/\lambda$ is $\frac{\sum_{i=1}^{n}x_n}n$ which is the sample mean.
```{r}
set.seed(12345)
options(warn = -1)
library(boot)
B <- 2000                                                         #bootstrap number
stat <- function(dat, index){mean(dat[index])}                    #function
bootstrap_result <- boot(data = aircondit$hours, stat, R = B)     #boot result
boot.ci(bootstrap_result, conf = 0.95, type = c("norm", "basic", "perc", "bca")) 
```
We can see that the interval of bca method is larger than the three methods. This is because the interval tends to be too narrow for small n with normal and basic methods.

We get a warning message that BCa intervals may be unstable. This is because the accuracy of the bias and acceleration terms require a large number of bootstrap samples and this can be computationally intensive.

## Question3
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

## Answer
```{r}
data(scor, package = "bootstrap") #import data
n <- nrow(scor)
lambda_hat <- eigen((n-1)*cov(scor)/n)$values
theta_hat <- lambda_hat[1]/sum(lambda_hat)
theta_j <- rep(0, n) 
for (i in 1:n) { 
  x <- scor [-i,]
  m <- n-1
  lambda <- eigen((m-1)*cov(x)/m)$values 
  theta_jack[i] <- lambda[1] / sum(lambda)   
  }
bias <- (n - 1) * (mean(theta_jack) - theta_hat)
se <- (n - 1) * sqrt(var(theta_jack) / n)
df2 <- data.frame(theta_hat, bias, se)
knitr::kable(df2)

```

## Question4
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

## Answer
```{r}
data(ironslag, package = "DAAG")
magnetic <- ironslag$magnetic
chemical <- ironslag$chemical
n <- nrow(ironslag) 
e1 <- e2 <- e3 <- e4 <- matrix(0,n,n)
# for n-fold cross validation
# fit models on leave-two-out samples
for (k in 1:n) {
  for(l in 1:n){
    y <- magnetic[c(-k,-l)]
    x <- chemical[c(-k,-l)]
    
    J1 <- lm(y ~ x)
    yhat11 <- J1$coef[1] + J1$coef[2] * chemical[k]
    yhat12 <- J1$coef[1] + J1$coef[2] * chemical[l]
    e1[k,l] <- ((magnetic[k] - yhat11)^2 + (magnetic[l] - yhat12)^2)/2       #mean error
    
    J2 <- lm(y ~ x + I(x^2))
    yhat21 <- J2$coef[1] + J2$coef[2] * chemical[k] + J2$coef[3] * chemical[k]^2
    yhat22 <- J2$coef[1] + J2$coef[2] * chemical[l] + J2$coef[3] * chemical[l]^2
    e2[k,l] <- ((magnetic[k] - yhat21)^2 + (magnetic[l] - yhat22)^2)/2      #mean error
     
    J3 <- lm(log(y) ~ x)
    logyhat31 <- J3$coef[1] + J3$coef[2] * chemical[k]
    logyhat32 <- J3$coef[1] + J3$coef[2] * chemical[l]
    yhat31 <- exp(logyhat31)
    yhat32 <- exp(logyhat32)
    e3[k,l] <- ((magnetic[k] - yhat31)^2 + (magnetic[l] - yhat32)^2)/2       #mean error
    
    J4 <- lm(log(y) ~ log(x))
    logyhat41 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
    logyhat42 <- J4$coef[1] + J4$coef[2] * log(chemical[l])
    yhat41 <- exp(logyhat41)
    yhat42 <- exp(logyhat42)
    e4[k,l] <- ((magnetic[k] - yhat41)^2 + (magnetic[l] - yhat42)^2)/2       #mean error
  }
}
df3 <- data.frame(mean(e1), mean(e2), mean(e3), mean(e4))
names(df3)=c("mean1", "mean2", "mean3", "mean4")
knitr::kable(df3)
```

According to the predction error criterion, Model 2 would be the best fit for the data. Below is the fitted regression equation and residual plots.

```{r}
L2 <- lm(magnetic ~ chemical + I(chemical^2))
L2
plot(L2$fit, L2$res)            #residuals vs fitted values
abline(0, 0)                    #reference line 
qqnorm(L2$res)                  #qq plot
qqline(L2$res)
```


## Homework-11.10
## Question1
The Count 5 test for equal variances in Section 6.4 is based on the maximum
number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal.

## Answer
Gather two samples together and randomly split the sample into two part equally to conduct Count 5 test.

```{r}
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))       
  return(as.integer(max(c(outx, outy)) > 5))        # return 1 (reject) or 0 (do not reject H0)
}

set.seed(12345)                                     # seed
n1 <- 20 
n2 <- 30
mu1 <- mu2 <-0
sigma1 <- sigma2 <-1
x <- rnorm(n1, mu1, sigma1)                         # sample generation
y <- rnorm(n2, mu2, sigma2)
z <- c(x, y)
R <- 1e4                                            # cycle times
n <- (n1 + n2) / 2
m <- c(1 : (n1 + n2))

res <- numeric(R)                                   # storage 
for (i in 1:R) {                                    # loop
  k <- sample(m, size = n, replace = FALSE)         # split
  x <- z[k]
  y <- z[-k]
  res[i] <- count5test(x, y)
}
mean(res)
```

Note that type1 error is close to $0.055$. We confirm the fact that the permutaion method is effective.

## Question2
Design experiments for evaluating the performance of the NN,
energy, and ball methods in various situations:

(1) Unequal variances and equal expectations.

(2) Unequal variances and unequal expectations.

(3) Non-normal distributions: $t$ distribution with 1 df (heavy-tailed
distribution), bimodel distribution (mixture of two normal
distributions).

(4) Unbalanced samples (say, 1 case versus 10 controls).



## Answer
```{r}
options(warn = -1)
library(RANN)
library(boot)
library(energy)
library(Ball)


m <- 50; k<-3; p<-2; 
n1 <- n2 <- 50; R<-999; n <- n1+n2; N = c(n1,n2)

Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1) # what's the first column?
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}

eqdist.nn <- function(z,sizes,k){
  boot.obj <- boot(data=z,statistic=Tn,R=R,sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}

p.values <- matrix(NA,m,3)
for(i in 1:m){
  x <- matrix(rnorm(n1*p,mean = 1,sd = 0.6),ncol=p);
  y <- matrix(rnorm(n2*p,mean = 1,sd = 0.85),ncol=p);
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,R=999,seed = i*12)$p.value
}
alpha <- 0.05;
pow <- colMeans(p.values<alpha)
print(pow)
barplot(pow,col = "lightgreen",main = "Unequal variances and unequal expectations",
        names.arg=c("NN","energy","ball"))
```

Ball method performs best in the unequal variances and unequal expectations circumstance.
```{r}
m <- 50; k<-3; p<-2; 
n1 <- n2 <- 50; R<-999; n <- n1+n2; N = c(n1,n2)
p.values <- matrix(NA,m,3)
for(i in 1:m){
  x <- matrix(rnorm(n1*p,mean = 0.4,sd = 0.6),ncol=p);
  y <- matrix(rnorm(n2*p,mean = 0.5,sd = 0.85),ncol=p);
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,R=999,seed = i*12)$p.value
}
alpha <- 0.05;
pow <- colMeans(p.values<alpha)
print(pow)
barplot(pow,col = "lightblue",main = "Unequal variances and equal expectations",
        names.arg=c("NN","energy","ball"))
```

Ball method performs best in the unequal variances and equal expectations circumstance.

```{r}
m <- 50; k<-3; p<-2; 
n1 <- n2 <- 20; R<-999; n <- n1+n2; N = c(n1,n2)
p.values <- matrix(NA,m,3)
for(i in 1:m){
  # t distribution with 1 df (heavy-tailed distribution)
  x <- matrix(rt(n1*p,df = 1),ncol=p); 
  #bimodel distribution (mixture of two normal distributions)
  y <- cbind(rnorm(n2,mean = 0.4),rnorm(n2,mean = 0.5));
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,R=999,seed = i*12)$p.value
}
alpha <- 0.05;
pow <- colMeans(p.values<alpha)
print(pow)
barplot(pow,col = "pink",main = "Non-normal distributions",
        names.arg=c("NN","energy","ball"))
```

Energy and ball methods perform best in the non-normal distributions circumstance.

```{r}
m <- 50; k<-3; p<-2; 
n1 <- 10;n2 <- 100;R<-999; n <- n1+n2; N = c(n1,n2)
p.values <- matrix(NA,m,3)
for(i in 1:m){
  x <- c(rnorm(n1,mean = 1,sd = 1)); # n1 = 10
  y <- c(rnorm(n2,mean = 2,sd = 2)); # n2 = 100
  z <- c(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,R=999,seed = i*12)$p.value
}
alpha <- 0.05;
pow <- colMeans(p.values<alpha)
print(pow)
barplot(pow,col = "lightgreen",main = "Unbalanced sample",names.arg=c("NN","energy","ball"))
```

Ball method performs best in the unbalanced sample circumstance.

## Homework-11.17
## Question1
Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.

## Answer
```{r}
options(warn = -1)
library(GeneralizedHyperbolic)
Laplace.Metropolis <- function(sigma, N){
  x <- numeric(N)
  x[1] <- rnorm(1,0,sigma)
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if(u[i] <= dskewlap(y)/dskewlap(x[i-1]))
      x[i] <- y
    else{
      x[i] <- x[i-1]
      k <- k+1
    }
  }
  return(list(x = x, k = k))
}
N <- 1e4
sigma <- c(1, 4, 9, 16)
set.seed(1234)
laplace1<-Laplace.Metropolis(sigma[1], N)
laplace2<-Laplace.Metropolis(sigma[2], N)
laplace3<-Laplace.Metropolis(sigma[3], N)
laplace4<-Laplace.Metropolis(sigma[4], N)
df <- data.frame(c(1-laplace1$k/N,1-laplace2$k/N,1-laplace3$k/N,1-laplace4$k/N))
names(df) <- c("acceptance rate")
rownames(df) <- c("sigma=1", "sigma=4", "sigma=9", "sigma=16")
knitr::kable(df)
```

We can see that the acceptance rate decreased while the variance increased.

## Question2
For Exercise 9.4, use the Gelman-Rubin method to monitor
convergence of the chain, and run the chain until it converges
approximately to the target distribution according to $\hat{R}<1.2$.

## Answer
```{r}
Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)
psi.means <- rowMeans(psi) #row means
B <- n * var(psi.means) #between variance est.
psi.w <- apply(psi, 1, "var") #within variances
W <- mean(psi.w) #within est.
v.hat <- W*(n-1)/n + (B/n) #upper variance est.
r.hat <- v.hat / W #G-R statistic
return(r.hat)
}
Laplace.Metropolis <- function(sigma, N, X1){
  x <- numeric(N)
  x[1] <- X1
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1,x[i-1],sigma)
    if(u[i] <= dskewlap(y)/dskewlap(x[i-1]))
      x[i] <- y
    else{
      x[i] <- x[i-1]
      k <- k+1
    }
  }
  return(x)
}
k <- 4 #number of chains to generate
n <- 15000
b <- 1000
sigma <- 1
x0 <- c(-10, -5, 5, 10)
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
X[i, ] <- Laplace.Metropolis(sigma, n, x0[i])

psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))
#plot psi for the four chains
for (i in 1:k)
plot(psi[i, (b+1):n], type="l",
xlab=i, ylab=bquote(psi))
par(mfrow=c(1,1)) #restore default
#plot the sequence of R-hat statistics
rhat <- rep(0,n)
for (j in (b+1):n)
rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```

We can see that the chain has approximately converged to the target distribution within approximately 3500 iterations when sigma=1.

## Question3
Find the intersection points $A(k)$ in $(0,\sqrt{k})$ of the curves
$$S_{k-1}(a)=P( t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}} )$$
and
$$S_{k}(a)=P( t(k)>\sqrt{\frac{a^2k}{k+1-a^2}} ),$$
for $k=4:25,100,500,1000,$ where $t(k)$ is a Student $t$ random variable with $k$ degrees of freedom. (These intersection points determine the critical values for a t-test for scale-mixture errors proposed by $Sz\acute{e}kely$ [260].) 

## Answer
Use uniroot function to get the intersection points' abscissa.

```{r}
u <- c(seq(4,25),100,500,1000)
b <- numeric(length(u))
c <- numeric(length(u))
s1 <- function(a){   #S_k-1
  1-pt(sqrt(a^2*(k-1)/(k-a^2)),k-1)
}
s2 <- function(a){   #S_k
  1-pt(sqrt(a^2*k/(k+1-a^2)),k)
}
s <- function(a){   #s1-s2
  1-pt(sqrt(a^2*(k-1)/(k-a^2)),k-1)-(1-pt(sqrt(a^2*k/(k+1-a^2)),k))
}
for(i in 1:length(u)){   
  k <- u[i]
  b[i] <- uniroot(s,c(0.01,2))$root    #get root
  c[i] <- s1(b[i])
}

df2 <- data.frame(u,b,c)
names(df2) <- c("k", "abscissa", "Y-axis")
knitr::kable(df2)
```

## Homework-11.24
## Question1
+ A-B-O blood type problem
+ Let the three alleles be A, B, and O.
        ```{r,echo=FALSE}
        options(warn=-1)
        dat <- rbind(Genotype=c('AA','BB','OO','AO','BO','AB','Sum'),
                     Frequency=c('p^2','q^2','r^2','2pr','2qr','2pq',1),
                     Count=c('nAA','nBB','nOO','nAO','nBO','nAB','n'))
        knitr::kable(dat)
        ```
+ Observed data: $n_{A\cdot}=n_{AA}+n_{AO}=444$ (A-type), $n_{B\cdot}=n_{BB}+n_{BO}=132$ (B-type), $n_{OO}=361$ (O-type), $n_{AB}=63$ (AB-type).
    
+ Use EM algorithm to solve MLE of $p$ and $q$ (consider missing data $n_{AA}$ and $n_{BB}$).
    
+ Record the values of p and q that maximize the conditional likelihood in each EM steps, calculate the corresponding log-maximum likelihood values (for observed data), are they increasing?

## Answer
We can obtain the log-likelihood which is $$l(p,q|n_{AA},n_{BB},n_{OO},n_{A.},n_{B.},n_{AB})=2n_{AA}log(p)+2n_{BB}log(q)+2n_{OO}log(r)+(n_{A.}-n_{AA})log(2pr)+(n_{B.}-n_{BB})log(2qr)+n_{AB}log(2pq), $$ where $r=1-p-q$.

```{r}
p0 <- 0.5; q0 <- 0.3                                   # initial value
nA <- 444; nB <- 132; nOO <- 361; nAB <- 63            # data
tol <- .Machine$double.eps^0.5                         # tolerance
N <- 10000                                                            
p <- p0; q <- q0
logl <- numeric(N)                                     # log-likelihood                       
k <- 1 
L.old <- c(p0, q0)
for (i in 1:N) {
  r <- 1-p-q
  a <- p/(2-p-2*q)
  b <- q/(2-2*p-q)
  Hp <- nA*(1+a)+nAB
  Hq <- nB*(1+b)+nAB
  Hr <- nA*(1-a)+nB*(1-b)+2*nOO
  H2 <- nA*(1-a)+nB*(1-b)+nAB
  logl[i] <- Hp*log(p)+Hq*log(q)+Hr*log(r)+H2*log(2)    # log-likelihood
  p.old <- p; q.old <- q                
  p <- Hp/(Hp+Hq+Hr); q <- Hq/(Hp+Hq+Hr)                # update
  L <- c(p, q)
  if (sum(abs(L-L.old)/L.old)<tol) break
  L.old <- L
  k <- k+1
}  
p.hat<-p; q.hat<-q
logl<-logl[1:k]
df <- data.frame(c(p.hat,q.hat))
names(df) <- "value"
rownames(df) <- c("p.hat", "q.hat")
knitr::kable(df)
plot(c(1:length(logl)), logl, type = "b", xlab = "iteration", ylab = " log-likelihood")
```

We can see that the EM algorithm converged in $12$ iterations (within $< 1.5e-8$) to the estimate and the log-likelihood values are increasing.

## Question2
 
Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:

formulas <- list( 

mpg ~ disp,

mpg ~ I(1 / disp),

mpg ~ disp + wt,

mpg ~ I(1 / disp) + wt

)

### Answer

```{r}

attach(mtcars)

formulas = list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
#1 for loops
f3 = vector("list", length(formulas))
for (i in seq_along(formulas)){
  f3[[i]] = lm(formulas[[i]], data = mtcars)
}
f3
#2 lapply
la3 = lapply(formulas, function(x) lm(formula = x, data = mtcars))
la3

``` 

## Question3
The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.

```{r}
trials <- replicate(
  100, 
  t.test(rpois(10, 10), rpois(7, 10)), 
  simplify = FALSE 
) 
```

Extra challenge: get rid of the anonymous function by using [[ directly.

## Answer

```{r}
# anonymous
sapply(trials, function(x) x$p.value)
# without anonymous function
sapply(trials, '[[', "p.value")
```

## Question4
Implement a combination of Map() and vapply() to create an
lapply() variant that iterates in parallel over all of its inputs
and stores its outputs in a vector (or a matrix). What arguments should the function take?

## Answer
```{r}
Mvap <- function(x, fun, value){
  out <- Map(function(y) vapply(y, fun, value), x)
  unlist(out)
}
options(warn = -1)
testlist <- list(cars, faithful, iris)
Mvap(testlist, mean, numeric(1))
```

## Homework-12.1
## Question
+ Write an Rcpp function for Exercise 9.4 (page 277, Statistical
Computing with R).
+ Compare the corresponding generated random numbers with
those by the R function you wrote before using the function
“qqplot”.
+ Campare the computation time of the two functions with the
function “microbenchmark”.
+ Comments your results.

## Answer
```{r}
library(Rcpp)
library(GeneralizedHyperbolic)
library(microbenchmark)
f <- function(x) 0.5*exp(-abs(x))
rwMetropolosR <- function(sigma, x0, N){
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= f(y)/f(x[i-1]))
      x[i] <- y 
    else {
      x[i] <- x[i-1]
      k <- k+1
    }
  }
  return(list(x = x,k = k))
}
cppFunction('NumericVector rwMetropolosC(double sigma, double x0, int N){
  NumericVector x(N+1);
  double k=0;
  x[0]=x0;
  double y=0;
  NumericVector u(N);
  for (int i=0;i<N;i++){
    u[i]=runif(1)[0];
  }
  for (int j=1;j<N-1;j++){
    y=rnorm(1,x[j-1],sigma)[0];
    if (u[j]<=0.5*exp(-abs(y))/(0.5*exp(-abs(x[j-1]))))
      x[j]=y;
    else {
      x[j]=x[j-1];
      k++;
    }
  }
  x[N]=k;
  return x;
}')
N <- 2e3
set.seed(1234)
sigma <- c(.05, .5, 2, 16)
x0 <- 25
#rwMetropolosR() returns a list with x and k, with the sample vector and the rejection number. 
rwR1 <- rwMetropolosR(sigma[1],x0,N)
rwR2 <- rwMetropolosR(sigma[2],x0,N)
rwR3 <- rwMetropolosR(sigma[3],x0,N)
rwR4 <- rwMetropolosR(sigma[4],x0,N)
#rwMetropolosC() returns a vector x, with the first N the sample and the last the rejection number.
rwC1 <- rwMetropolosC(sigma[1],x0,N)
rwC2 <- rwMetropolosC(sigma[2],x0,N)
rwC3 <- rwMetropolosC(sigma[3],x0,N)
rwC4 <- rwMetropolosC(sigma[4],x0,N)
refline <- qskewlap(c(.025, .975))
rwR <- list(rwR1,rwR2,rwR3,rwR4)
rwC <- list(rwC1,rwC2,rwC3,rwC4)
for (i in 1:4){
  plot(1:2000, rwR[[i]]$x, type = "l", xlab = paste("sigma=",sigma[i]), ylab = "x", main = "Using R codes")
  abline(h=refline)
}
for (i in 1:4) {
  plot(1:2000, rwC[[i]][1:2000], type = "l", xlab = paste("sigma=",sigma[i]), ylab = "x", main = "Using Rcpp")
  abline(h=refline)
}
#acceptance rate with rwMetropolosR
print(c(1-rwR1$k/N, 1-rwR2$k/N, 1-rwR3$k/N, 1-rwR4$k/N))
#acceptance rate with rwMetropolosC
print(c(1-rwC1[N+1]/N, 1-rwC2[N+1]/N, 1-rwC3[N+1]/N, 1-rwC4[N+1]/N))
```

We can see that two functions get similiar results.

```{r}
#qqplot
a <- ppoints(100)
for (i in 1:4) {
  QR <- qskewlap(a)
  Q <- quantile(rwR[[i]]$x,a)
  qqplot(QR, Q, main=paste("Using R codes with sigma=",sigma[i]),xlab = "standard Laplace Quantiles", ylab = "Sample Quantiles")
  lines(QR,QR)
}
for (i in 1:4) {
  QR <- qskewlap(a)
  Q <- quantile(rwC[[i]][1:2000],a)
  qqplot(QR, Q, main=paste("Using Rcpp with sigma=",sigma[i]), xlab = "standard Laplace Quantiles", ylab = "Sample Quantiles")
  lines(QR,QR)
}
```

The qqplots with R and Rcpp are similiar. We can see that when sigma gets larger, the points are closer to the line.

```{r}
summary <- list(4)
for (i in 1:4) {
  ts <- microbenchmark(rwR[[i]] <- rwMetropolosR(sigma[i],x0,N), rwC[[i]] <- rwMetropolosC(sigma[i],x0,N))
  summary[[i]]<-list(paste0('sigma=',sigma[i]),summary(ts)[,c(1,3,5,6)])
}
print(summary)
```

We can see that the function written by Rcpp is way more faster than the function written by R.


